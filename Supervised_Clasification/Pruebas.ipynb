{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import glob\n",
    "import os\n",
    "# Especifica la ruta a la carpeta que contiene subcarpetas\n",
    "ruta_carpeta = r\"D:\\MDA\\Cucunuba\"\n",
    "\n",
    "# Utiliza glob para buscar todos los archivos .las de forma recursiva\n",
    "archivos_las = glob.glob(f'{ruta_carpeta}/**/*.las', recursive=True)\n",
    "\n",
    "# Imprime la lista de archivos encontrados\n",
    "for archivo in archivos_las:\n",
    "    print(archivo)\n",
    "    dtm=archivo.replace(\"ground.las\", \"dtm.tif\")\n",
    "    print(dtm)\n",
    "    arcpy.conversion.LasDatasetToRaster(\n",
    "        in_las_dataset=archivo,\n",
    "        out_raster=dtm,\n",
    "        value_field=\"ELEVATION\",\n",
    "        interpolation_type=\"BINNING MINIMUM NATURAL_NEIGHBOR\",\n",
    "        data_type=\"FLOAT\",\n",
    "        sampling_type=\"CELLSIZE\",\n",
    "        sampling_value=0.5,\n",
    "        z_factor=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO BORRAR Codigo para separar Vectores en el PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs separados guardados en el directorio: D:\\PROYECTOS 2024\\RENE\\Pacho 2024-06-20\\6.Reportes procesamiento\\Pacho_2024-06-20 GNSS Processing Report ROVER_separated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "def extract_baselines(input_pdf):\n",
    "    # Cargar el PDF\n",
    "    reader = PdfReader(input_pdf)\n",
    "    total_pages = len(reader.pages)\n",
    "\n",
    "    # Crear una lista para almacenar los índices de las páginas que comienzan con \"Baseline BASE\"\n",
    "    baseline_indices = []\n",
    "    for i in range(total_pages):\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        if text and \"Baseline 2042\" in text:\n",
    "            baseline_indices.append(i)\n",
    "\n",
    "    # Añadir el índice de la última página\n",
    "    baseline_indices.append(total_pages)\n",
    "\n",
    "    # Crear directorio para guardar los archivos PDF separados\n",
    "    output_dir = os.path.splitext(input_pdf)[0] + \"_separated\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Extraer y guardar cada sección en un archivo PDF separado\n",
    "    for j in range(len(baseline_indices) - 1):\n",
    "        writer = PdfWriter()\n",
    "        start_index = baseline_indices[j]\n",
    "        end_index = baseline_indices[j + 1]\n",
    "\n",
    "        for k in range(start_index, end_index):\n",
    "            writer.add_page(reader.pages[k])\n",
    "\n",
    "        output_pdf_path = os.path.join(output_dir, f\"Baseline_{j + 1}.pdf\")\n",
    "        with open(output_pdf_path, \"wb\") as output_pdf_file:\n",
    "            writer.write(output_pdf_file)\n",
    "\n",
    "    print(f\"PDFs separados guardados en el directorio: {output_dir}\")\n",
    "\n",
    "# Usar la función con tu archivo PDF\n",
    "extract_baselines(r\"D:\\PROYECTOS 2024\\RENE\\Pacho 2024-06-20\\6.Reportes procesamiento\\Pacho_2024-06-20 GNSS Processing Report ROVER.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas PPK Mavic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyexiftool in c:\\users\\nicolasviasus\\.conda\\envs\\mda\\lib\\site-packages (0.5.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pyexiftool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect metadata jpg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from exiftool import ExifToolHelper\n",
    "\n",
    "def extract_metadata_to_csv(folder_path, output_csv):\n",
    "    # Obtener la lista de todos los archivos JPEG en la carpeta\n",
    "    files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith('.jpg') or file.lower().endswith('.jpeg')]\n",
    "\n",
    "    # Lista para almacenar todos los metadatos\n",
    "    all_metadata = []\n",
    "\n",
    "    # Extraer metadatos de cada archivo\n",
    "    with ExifToolHelper() as et:\n",
    "        for file in files:\n",
    "            metadata = et.get_metadata(file)\n",
    "            for d in metadata:\n",
    "                cleaned_data = {k.split(\":\")[-1]: v for k, v in d.items()}\n",
    "                all_metadata.append(cleaned_data)\n",
    "\n",
    "    # Obtener todas las claves posibles para asegurarse de que el CSV tiene todas las columnas necesarias\n",
    "    all_keys = set()\n",
    "    for data in all_metadata:\n",
    "        all_keys.update(data.keys())\n",
    "    all_keys = sorted(all_keys)  # Ordenar las claves para un CSV ordenado\n",
    "\n",
    "    # Escribir los metadatos en el archivo CSV\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for data in all_metadata:\n",
    "            writer.writerow(data)\n",
    "\n",
    "# Ruta a la carpeta que contiene las fotos\n",
    "folder_path = r\"D:\\MDA\\Lomas de Turillas\\DJI_202407020813_003_Crear-ruta-de-zona5\"\n",
    "# Ruta al archivo CSV de salida\n",
    "output_csv = folder_path+\"\\metadata_img.csv\"\n",
    "\n",
    "# Llamar a la función para extraer los metadatos y exportarlos a CSV\n",
    "extract_metadata_to_csv(folder_path, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the closest UTC time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'D:\\MDA\\Lomas de Turillas\\DJI_202407020813_003_Crear-ruta-de-zona5\\prueba_PPK.csv' generado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def read_pos_file(pos_file_path):\n",
    "    \"\"\"Lee un archivo POS y devuelve un DataFrame con las trayectorias.\"\"\"\n",
    "    data = []\n",
    "    with open(pos_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if line.startswith('%'):\n",
    "                continue  # Saltar líneas de encabezado\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 14:  # Asumiendo que hay al menos 16 partes en los datos de trayectoria\n",
    "                utc_time = parts[0] + ' ' + parts[1]\n",
    "                # Convertir la cadena de tiempo a un objeto datetime\n",
    "                utc_time = datetime.datetime.strptime(utc_time, \"%Y/%m/%d %H:%M:%S.%f\")\n",
    "                data.append({\n",
    "                    'utc_time': utc_time,\n",
    "                    'lat': float(parts[2]),\n",
    "                    'lon': float(parts[3]),\n",
    "                    'height': float(parts[4]),\n",
    "                    'sdn': float(parts[7]),\n",
    "                    'sde': float(parts[8]),\n",
    "                    'sdu': float(parts[9]),\n",
    "                    # Agrega más columnas si es necesario\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def find_closest_time(photo_time, trajectory_times):\n",
    "    \"\"\"Encuentra la hora más cercana en la trayectoria para una hora de foto dada.\"\"\"\n",
    "    closest_time = min(trajectory_times, key=lambda x: abs(x - photo_time))\n",
    "    return closest_time\n",
    "\n",
    "def process_photos_and_trajectories(photos_csv_path, pos_file_path, output_csv_path):\n",
    "    \"\"\"Procesa las fotos y las trayectorias, y guarda las correlaciones en un archivo CSV.\"\"\"\n",
    "    # Leer los archivos\n",
    "    photos_metadata = pd.read_csv(photos_csv_path)\n",
    "    trajectory_data = read_pos_file(pos_file_path)\n",
    "\n",
    "    # Asegurarse de que las columnas de tiempo estén en formato datetime\n",
    "    photos_metadata['UTCAtExposure'] = pd.to_datetime(photos_metadata['UTCAtExposure'], format=\"%Y:%m:%d %H:%M:%S.%f\")\n",
    "    trajectory_data['utc_time'] = pd.to_datetime(trajectory_data['utc_time'])\n",
    "\n",
    "    # Crear una lista para almacenar los resultados\n",
    "    results = []\n",
    "\n",
    "    # Iterar sobre cada fila en los metadatos de las fotos\n",
    "    for index, row in photos_metadata.iterrows():\n",
    "        photo_name = row['FileName']\n",
    "        photo_time = row['UTCAtExposure']\n",
    "        \n",
    "        # Encontrar la fila correspondiente en los datos de trayectoria\n",
    "        closest_time = find_closest_time(photo_time, trajectory_data['utc_time'])\n",
    "        trajectory_row = trajectory_data.loc[trajectory_data['utc_time'] == closest_time]\n",
    "\n",
    "        # Extraer los valores de la trayectoria más cercana\n",
    "        lat = trajectory_row['lat'].values[0]\n",
    "        lon = trajectory_row['lon'].values[0]\n",
    "        height = trajectory_row['height'].values[0]\n",
    "        sdn = trajectory_row['sdn'].values[0]\n",
    "        sde = trajectory_row['sde'].values[0]\n",
    "        sdu = trajectory_row['sdu'].values[0]\n",
    "\n",
    "        # Guardar los resultados\n",
    "        results.append({\n",
    "            'photo_name': photo_name,\n",
    "            'photo_time': photo_time,\n",
    "            'closest_trajectory_time': closest_time,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'height': height,\n",
    "            'sdn': sdn,\n",
    "            'sde': sde,\n",
    "            'sdu': sdu\n",
    "        })\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Guardar los resultados en un nuevo archivo CSV\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Archivo '{output_csv_path}' generado con éxito.\")\n",
    "\n",
    "# Rutas a los archivos de entrada y salida\n",
    "photos_csv_path = r\"D:\\MDA\\Lomas de Turillas\\DJI_202407020813_003_Crear-ruta-de-zona5\\metadata_img.csv\"\n",
    "pos_file_path = r\"D:\\MDA\\Lomas de Turillas\\DJI_202407020813_003_Crear-ruta-de-zona5\\DJI_202407020813_003_Crear-ruta-de-zona5_PPKOBS.pos\"\n",
    "output_csv_path = r\"D:\\MDA\\Lomas de Turillas\\DJI_202407020813_003_Crear-ruta-de-zona5\\prueba_PPK.csv\"\n",
    "\n",
    "# Procesar las fotos y trayectorias, y guardar el archivo de salida\n",
    "process_photos_and_trajectories(photos_csv_path, pos_file_path, output_csv_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code complete V1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'D:\\MDA\\Lomas de Turillas\\DJI_202407020803_002_Crear-ruta-de-zona5\\Photos_PPK.csv' generado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from exiftool import ExifToolHelper\n",
    "\n",
    "def extract_metadata_to_csv(folder_path, output_csv):\n",
    "    # Obtener la lista de todos los archivos JPEG en la carpeta\n",
    "    files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "    # Lista para almacenar todos los metadatos\n",
    "    all_metadata = []\n",
    "\n",
    "    # Extraer metadatos de cada archivo\n",
    "    with ExifToolHelper() as et:\n",
    "        for file in files:\n",
    "            metadata = et.get_metadata(file)\n",
    "            for d in metadata:\n",
    "                cleaned_data = {k.split(\":\")[-1]: v for k, v in d.items()}\n",
    "                all_metadata.append(cleaned_data)\n",
    "\n",
    "    # Obtener todas las claves posibles para asegurarse de que el CSV tiene todas las columnas necesarias\n",
    "    all_keys = set()\n",
    "    for data in all_metadata:\n",
    "        all_keys.update(data.keys())\n",
    "    all_keys = sorted(all_keys)  # Ordenar las claves para un CSV ordenado\n",
    "\n",
    "    # Escribir los metadatos en el archivo CSV\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for data in all_metadata:\n",
    "            writer.writerow(data)\n",
    "\n",
    "# Ruta a la carpeta que contiene las fotos\n",
    "folder_path = r\"D:\\MDA\\Lomas de Turillas\\DJI_202407020803_002_Crear-ruta-de-zona5\"\n",
    "# Ruta al archivo CSV de salida\n",
    "output_csv = os.path.join(folder_path, \"metadata_img.csv\")\n",
    "\n",
    "# Llamar a la función para extraer los metadatos y exportarlos a CSV\n",
    "extract_metadata_to_csv(folder_path, output_csv)\n",
    "\n",
    "\n",
    "\"\"\" $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\"\"\"\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def find_pos_file(directory):\n",
    "    \"\"\"Encuentra el archivo .pos en el directorio dado.\"\"\"\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.pos'):\n",
    "            return os.path.join(directory, file_name)\n",
    "    raise FileNotFoundError(\"No se encontró un archivo .pos en el directorio especificado.\")\n",
    "\n",
    "def read_pos_file(pos_file_path):\n",
    "    \"\"\"Lee un archivo POS y devuelve un DataFrame con las trayectorias.\"\"\"\n",
    "    data = []\n",
    "    with open(pos_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines[24:]:  # Saltar las primeras 24 líneas de encabezado\n",
    "            if not line.startswith('%'):  # Saltar líneas de encabezado\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 14:  # Asumiendo que hay al menos 14 partes en los datos de trayectoria\n",
    "                    utc_time = parts[0] + ' ' + parts[1]\n",
    "                    # Convertir la cadena de tiempo a un objeto datetime\n",
    "                    utc_time = datetime.datetime.strptime(utc_time, \"%Y/%m/%d %H:%M:%S.%f\")\n",
    "                    data.append({\n",
    "                        'utc_time': utc_time,\n",
    "                        'lat': float(parts[2]),\n",
    "                        'lon': float(parts[3]),\n",
    "                        'height': float(parts[4]),\n",
    "                        'sdn': float(parts[7]),\n",
    "                        'sde': float(parts[8]),\n",
    "                        'sdu': float(parts[9]),\n",
    "                        # Agrega más columnas si es necesario\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def find_closest_time(photo_time, trajectory_times):\n",
    "    \"\"\"Encuentra la hora más cercana en la trayectoria para una hora de foto dada.\"\"\"\n",
    "    closest_time = min(trajectory_times, key=lambda x: abs(x - photo_time))\n",
    "    return closest_time\n",
    "\n",
    "def process_photos_and_trajectories(photos_csv_path, pos_directory, output_csv_path):\n",
    "    \"\"\"Procesa las fotos y las trayectorias, y guarda las correlaciones en un archivo CSV.\"\"\"\n",
    "    # Encontrar el archivo .pos en el directorio especificado\n",
    "    pos_file_path = find_pos_file(pos_directory)\n",
    "    \n",
    "    # Leer los archivos\n",
    "    photos_metadata = pd.read_csv(photos_csv_path)\n",
    "    trajectory_data = read_pos_file(pos_file_path)\n",
    "\n",
    "    # Asegurarse de que las columnas de tiempo estén en formato datetime\n",
    "    photos_metadata['UTCAtExposure'] = pd.to_datetime(photos_metadata['UTCAtExposure'], format=\"%Y:%m:%d %H:%M:%S.%f\")\n",
    "    trajectory_data['utc_time'] = pd.to_datetime(trajectory_data['utc_time'])\n",
    "\n",
    "    # Crear una lista para almacenar los resultados\n",
    "    results = []\n",
    "\n",
    "    # Iterar sobre cada fila en los metadatos de las fotos\n",
    "    for index, row in photos_metadata.iterrows():\n",
    "        photo_name = row['FileName']\n",
    "        photo_time = row['UTCAtExposure']\n",
    "        \n",
    "        # Encontrar la fila correspondiente en los datos de trayectoria\n",
    "        closest_time = find_closest_time(photo_time, trajectory_data['utc_time'])\n",
    "        trajectory_row = trajectory_data.loc[trajectory_data['utc_time'] == closest_time]\n",
    "\n",
    "        # Extraer los valores de la trayectoria más cercana\n",
    "        lat = trajectory_row['lat'].values[0]\n",
    "        lon = trajectory_row['lon'].values[0]\n",
    "        height = trajectory_row['height'].values[0]\n",
    "        sdn = trajectory_row['sdn'].values[0]\n",
    "        sde = trajectory_row['sde'].values[0]\n",
    "        sdu = trajectory_row['sdu'].values[0]\n",
    "\n",
    "        # Guardar los resultados\n",
    "        results.append({\n",
    "            'photo_name': photo_name,\n",
    "            'photo_time': photo_time,\n",
    "            'closest_trajectory_time': closest_time,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'height': height,\n",
    "            'sdn': sdn,\n",
    "            'sde': sde,\n",
    "            'sdu': sdu\n",
    "        })\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Guardar los resultados en un nuevo archivo CSV\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Archivo '{output_csv_path}' generado con éxito.\")\n",
    "\n",
    "# Rutas a los archivos de entrada y salida\n",
    "photos_csv_path = output_csv\n",
    "pos_directory = folder_path\n",
    "output_csv_path = os.path.join(folder_path, \"Photos_PPK.csv\")\n",
    "\n",
    "# Procesar las fotos y trayectorias, y guardar el archivo de salida\n",
    "process_photos_and_trajectories(photos_csv_path, pos_directory, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join CSV photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos unidos y guardados en: D:\\MDA\\Resguardo Cota\\Photos_PPK_Join.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos CSV\n",
    "carpeta_csv = r\"D:\\MDA\\Resguardo Cota\"\n",
    "\n",
    "# Lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar sobre los archivos en la carpeta\n",
    "for archivo in os.listdir(carpeta_csv):\n",
    "    if archivo.endswith('.csv'):\n",
    "        ruta_completa = os.path.join(carpeta_csv, archivo)\n",
    "        df = pd.read_csv(ruta_completa)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_concatenado = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame concatenado en un nuevo archivo CSV\n",
    "ruta_salida = carpeta_csv+\"\\Photos_PPK_Join.csv\"\n",
    "df_concatenado.to_csv(ruta_salida, index=False)\n",
    "\n",
    "print(f'Archivos unidos y guardados en: {ruta_salida}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo CSV tiene 781590 registros.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\NicolasViasus\\Documents\\Temporales_NV\\Consolidado_Agua\\Consolidado_Posgress.csv\",encoding='Latin1')\n",
    "\n",
    "# Contar el número de registros (filas) en el DataFrame\n",
    "numero_registros = len(df)\n",
    "\n",
    "print(f\"El archivo CSV tiene {numero_registros} registros.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['muestra', 'informe', 'cliente', 'programa', 'programa_armonizado',\n",
       "       'municipio', 'fechamuestreo', 'año', 'fecharecepcion', 'fechareporte',\n",
       "       'punto', 'caudal', 'lluvia', 'tipo_de_agua', 'tipo_de_muestreo',\n",
       "       'hora_de_toma', 'nortey', 'estex', 'altura_msnm',\n",
       "       'parametro_armonizado', 'unidades', 'metodo', 'tipo_de_limite',\n",
       "       'limite', 'valor', 'id', 'latitud', 'longitud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
